diff --git a/spot_detection/losses/L2_norm.py b/spot_detection/losses/L2_norm.py
index fcd2c68..c567dd5 100644
--- a/spot_detection/losses/L2_norm.py
+++ b/spot_detection/losses/L2_norm.py
@@ -1,6 +1,6 @@
 import tensorflow.keras.backend as K
 import tensorflow as tf
-from f1_score import f1_score_loss
+from .f1_score import f1_score_loss, f1_score
 
 
 def l2_norm(y_true, y_pred):
diff --git a/spot_detection/models/base.py b/spot_detection/models/base.py
index f0bc453..267e48f 100644
--- a/spot_detection/models/base.py
+++ b/spot_detection/models/base.py
@@ -26,8 +26,8 @@ class Model:
         train_args: Dict,
         dataset_args: Dict,
         network_args: Dict,
-        batch_format_fn: Callable,
-        batch_augment_fn: Callable,
+        batch_format_fn: Callable = None,
+        batch_augment_fn: Callable = None,
     ):
         self.name = f"{DATESTRING}_{self.__class__.__name__}_{dataset_cls.__name__}_{network_fn.__name__}"
 
@@ -91,7 +91,7 @@ class Model:
             validation_data=valid_sequence,
             # use_multiprocessing=False,
             # workers=1,
-            shuffle=True,
+            #shuffle=True,
         )
 
     def evaluate(self, x: np.ndarray, y: np.ndarray) -> float:
diff --git a/spot_detection/networks/fcn_spot.py b/spot_detection/networks/fcn_spot.py
index ab8bbf2..2688321 100644
--- a/spot_detection/networks/fcn_spot.py
+++ b/spot_detection/networks/fcn_spot.py
@@ -8,33 +8,26 @@ def fcn(n_channels: int = 3) -> tf.keras.models.Model:
 
     i = 5
 
-    inputs = tf.keras.layers.Input(shape=(512, 512, n_channels))
+    inputs = tf.keras.layers.Input(shape=(512, 512, 1))
 
     # Down: 512 -> 256
-    x = tf.keras.layers.Conv2D(filters=2**(i), strides=1, **OPTIONS_CONV)(inputs)
+    x = tf.keras.layers.Conv2D(filters=2 ** (i), strides=1, **OPTIONS_CONV)(inputs)
     x = tf.keras.layers.Activation("relu")(x)
-    x = tf.keras.layers.Conv2D(filters=2**(i), strides=1, **OPTIONS_CONV)(x)
+    x = tf.keras.layers.Conv2D(filters=2 ** (i), strides=1, **OPTIONS_CONV)(x)
     x = tf.keras.layers.Activation("relu")(x)
     x = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(x)
 
     # Down: 256 -> 128
-    x = tf.keras.layers.Conv2D(filters=2**(i+1), strides=1, **OPTIONS_CONV)(x)
+    x = tf.keras.layers.Conv2D(filters=2 ** (i + 1), strides=1, **OPTIONS_CONV)(x)
     x = tf.keras.layers.Activation("relu")(x)
-    x = tf.keras.layers.Conv2D(filters=2**(i+1), strides=1, **OPTIONS_CONV)(x)
-    x = tf.keras.layers.Activation("relu")(x)
-    x = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(x)
-
-    # Down: 256 -> 128
-    x = tf.keras.layers.Conv2D(filters=2**(i+2), strides=1, **OPTIONS_CONV)(x)
-    x = tf.keras.layers.Activation("relu")(x)
-    x = tf.keras.layers.Conv2D(filters=2**(i+2), strides=1, **OPTIONS_CONV)(x)
+    x = tf.keras.layers.Conv2D(filters=2 ** (i + 1), strides=1, **OPTIONS_CONV)(x)
     x = tf.keras.layers.Activation("relu")(x)
     x = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(x)
 
     # Connected
-    x = tf.keras.layers.Conv2D(filters=2**(i+3), strides=1, **OPTIONS_CONV)(x)
+    x = tf.keras.layers.Conv2D(filters=2 ** (i + 3), strides=1, **OPTIONS_CONV)(x)
     x = tf.keras.layers.Activation("relu")(x)
-    x = tf.keras.layers.Conv2D(filters=2**(i+3), strides=1, **OPTIONS_CONV)(x)
+    x = tf.keras.layers.Conv2D(filters=2 ** (i + 3), strides=1, **OPTIONS_CONV)(x)
     x = tf.keras.layers.Activation("relu")(x)
     x = tf.keras.layers.Conv2D(filters=3, kernel_size=1, strides=1)(x)
     x = tf.keras.layers.Activation("sigmoid")(x)
diff --git a/tasks/run_experiment.sh b/tasks/run_experiment.sh
index 8f36a53..072293c 100644
--- a/tasks/run_experiment.sh
+++ b/tasks/run_experiment.sh
@@ -1,4 +1,4 @@
 #!/bin/bash
 python ../training/run_experiment.py\
     --gpu 0\
-    --config ../training/experiments/nuclei_borders.yaml
+    --config ../training/experiments/spots.yaml 
diff --git a/training/util_training.py b/training/util_training.py
index 11d6af2..8b090f4 100644
--- a/training/util_training.py
+++ b/training/util_training.py
@@ -1,5 +1,5 @@
-from image_segmentation.datasets.dataset import Dataset
-from image_segmentation.models.base import Model
+from spot_detection.datasets.dataset import Dataset
+from spot_detection.models.base import Model
 import importlib
 import platform
 import sys
@@ -7,6 +7,7 @@ import tensorflow as tf
 import time
 import wandb
 from typing import Dict
+
 sys.path.append("../")
 
 
@@ -43,8 +44,9 @@ class WandbImageLogger(tf.keras.callbacks.Callback):
         wandb.log({"Ground truth": ground_truth}, commit=False)
 
         predictions = [
-            wandb.Image(self.model_wrapper.predict_on_image(
-                image), caption=f"Prediction: {i}")
+            wandb.Image(
+                self.model_wrapper.predict_on_image(image), caption=f"Prediction: {i}"
+            )
             for i, image in enumerate(self.valid_images)
         ]
         wandb.log({"Predictions": predictions}, commit=False)
@@ -54,10 +56,11 @@ def train_model(model: Model, dataset: Dataset) -> Model:
     """ Model training with wandb callbacks. """
 
     wandb_callback = wandb.keras.WandbCallback()
-    image_callback = WandbImageLogger(model, dataset)
+    # image_callback = WandbImageLogger(model, dataset)
     saver_callback = tf.keras.callbacks.ModelCheckpoint(
-        f"../models/model_{int(time.time())}.h5", save_best_only=False,)
-    callbacks = [wandb_callback, image_callback, saver_callback]
+        f"../models/model_{int(time.time())}.h5", save_best_only=False,
+    )
+    callbacks = [wandb_callback, saver_callback]
 
     tic = time.time()
     _history = model.fit(dataset=dataset, callbacks=callbacks)
@@ -88,14 +91,11 @@ def run_experiment(cfg: Dict, save_weights: bool = False):
         - save_weights: If model weights should be saved.
     """
 
-    dataset_class_ = get_from_module(
-        "image_segmentation.datasets", cfg["dataset"])
-    model_class_ = get_from_module("image_segmentation.models", cfg["model"])
-    network_fn_ = get_from_module(
-        "image_segmentation.networks", cfg["network"])
-    optimizer_fn_ = get_from_module(
-        "image_segmentation.optimizers", cfg["optimizer"])
-    loss_fn_ = get_from_module("image_segmentation.losses", cfg["loss"])
+    dataset_class_ = get_from_module("spot_detection.datasets", cfg["dataset"])
+    model_class_ = get_from_module("spot_detection.models", cfg["model"])
+    network_fn_ = get_from_module("spot_detection.networks", cfg["network"])
+    optimizer_fn_ = get_from_module("spot_detection.optimizers", cfg["optimizer"])
+    loss_fn_ = get_from_module("spot_detection.losses", cfg["loss"])
 
     network_args = cfg.get("network_args", {})
     dataset_args = cfg.get("dataset_args", {})
@@ -111,13 +111,13 @@ def run_experiment(cfg: Dict, save_weights: bool = False):
         optimizer_fn=optimizer_fn_,
         train_args=train_args,
         dataset_args=dataset_args,
-        network_args=network_args
+        network_args=network_args,
     )
 
     cfg["system"] = {
         "gpus": tf.config.list_logical_devices("GPU"),
         "version": platform.version(),
-        "platform": platform.platform()
+        "platform": platform.platform(),
     }
 
     wandb.init(project=cfg["name"], config=cfg)
diff --git a/wandb/settings b/wandb/settings
deleted file mode 100644
index 8dae664..0000000
--- a/wandb/settings
+++ /dev/null
@@ -1,2 +0,0 @@
-[default]
-
